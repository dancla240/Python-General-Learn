{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistik\n",
    "## Innehållsförteckning:\n",
    "- [Statistical Filter Methods](#statistical-filter-methods)\n",
    "- [Mutual Information](#mutual-information)\n",
    "- [Types of missingness](#types-of-missingness)\n",
    "- [Shannon Entropy](#shannon-entropy)\n",
    "- [Differential Entropy](#differential-entropy)\n",
    "- [Baye's Rule](#bayes-rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Filter Methods\n",
    "\n",
    "Below is an overview of **statistical filter methods** for feature selection in machine learning, focusing on their concepts, common techniques, and practical usage.\n",
    "\n",
    "## 1. What Is a Statistical Filter in Feature Selection?\n",
    "\n",
    "In machine learning, a **filter method** (often called a “statistical filter”) is a **preprocessing step** for feature selection that uses **statistical criteria** to measure how relevant each feature is to the prediction target. Features are then **ranked** or **selected** according to those statistics—**without** training a specific predictive model in the loop. \n",
    "\n",
    "### Key Idea\n",
    "- **Filter methods** separate the feature selection step from model building.\n",
    "- They use measures of association like correlation, mutual information, chi-square, etc.\n",
    "- They remove or select features **before** a learning algorithm sees the data.\n",
    "\n",
    "## 2. Why Use Statistical Filters for Feature Selection?\n",
    "\n",
    "1. **Efficiency**  \n",
    "   - They are typically **fast** and computationally inexpensive.  \n",
    "   - They don’t require training and evaluating multiple models for each feature subset.\n",
    "\n",
    "2. **Reduction in Overfitting**  \n",
    "   - By removing irrelevant or noisy features, you can often help the downstream model generalize better.\n",
    "\n",
    "3. **Improve Model Interpretability**  \n",
    "   - With fewer features, it’s easier to understand **which** input variables are most important and **why**.\n",
    "\n",
    "4. **Dimensionality Reduction**  \n",
    "   - In high-dimensional datasets, filter methods help reduce the dimensionality quickly without heavy computation.\n",
    "\n",
    "\n",
    "## 3. Common Statistical Filter Techniques\n",
    "\n",
    "1. **Correlation-Based Filtering**  \n",
    "   - **Pearson correlation** (for continuous features) or other correlation measures (e.g., Spearman, Kendall) are computed between each feature and the target.  \n",
    "   - Features that have a correlation magnitude below a certain threshold can be discarded.  \n",
    "   - Often used for quick screening, but only captures **linear** relationships.\n",
    "\n",
    "2. **Mutual Information (MI)**  \n",
    "   - Measures **any** (linear or nonlinear) dependence between each feature and the target.  \n",
    "   - A feature with higher MI to the target presumably carries more predictive signal.  \n",
    "   - In practice, can be more robust than correlation if relationships are nonlinear.\n",
    "\n",
    "3. **Chi-Square Test**  \n",
    "   - Common for **categorical** data. Compares observed vs. expected frequencies of a feature’s values across different classes of the target.  \n",
    "   - Features that show a strong dependency with the target (high chi-square score, low p-value) are selected.\n",
    "\n",
    "4. **ANOVA F-test**  \n",
    "   - Typically used for **continuous** features vs. **categorical** target (e.g., classification).  \n",
    "   - The F-test checks if the mean of a feature differs significantly across target classes.\n",
    "\n",
    "5. **Variance Threshold**  \n",
    "   - Simplest approach: remove all features whose variance is below some threshold.  \n",
    "   - Assumes features with low variance don’t carry much discriminative power.  \n",
    "   - Often used as a quick **first pass** to eliminate near-constant or trivially varying features.\n",
    "\n",
    "## 4. How These Methods Work in Practice\n",
    "\n",
    "1. **Compute the Score**  \n",
    "   - For each feature $X_i$, compute a statistical score with respect to the target $Y$. Examples:  \n",
    "     - Correlation coefficient,  \n",
    "     - Mutual Information,  \n",
    "     - Chi-square statistic,  \n",
    "     - F-statistic (ANOVA).\n",
    "\n",
    "2. **Rank Features**  \n",
    "   - Sort the features from **highest** to **lowest** in terms of the chosen score.\n",
    "\n",
    "3. **Select the Features**  \n",
    "   - **Keep** the top $k$ features or all features above some threshold.  \n",
    "   - This selection **does not** directly depend on how a specific model (like a neural network or SVM) would handle these features; it’s purely statistical.\n",
    "\n",
    "## 5. Advantages and Disadvantages of Statistical Filters\n",
    "\n",
    "### Advantages\n",
    "- **Speed and Scalability**: Easy to apply even with very large datasets (many rows or features).  \n",
    "- **Model-Agnostic**: Works independently of any ML algorithm you might use afterward.  \n",
    "- **Fewer Hyperparameters**: Usually just a threshold or a number $k$ of features to keep.\n",
    "\n",
    "### Disadvantages\n",
    "- **Ignores Feature Interactions**: Each feature is typically evaluated independently (univariate). Two features that individually have low relevance but combined have high predictive power might get discarded.  \n",
    "- **Not Always Optimal for the Final Model**: Because they are agnostic to the final model’s inner workings, filter methods might select suboptimal sets for certain algorithms that rely on complex interactions.\n",
    "\n",
    "\n",
    "## 6. Example Workflow\n",
    "\n",
    "Suppose you have a dataset with 1000 features (columns) and a target variable $Y$ (for classification).\n",
    "\n",
    "1. **Correlation Screening**  \n",
    "   - Compute the absolute Pearson correlation coefficient `abs_corr` between each feature and $Y$.  \n",
    "   - Discard any feature where `abs_corr < 0.1` (just an example threshold).\n",
    "\n",
    "2. **Mutual Information**  \n",
    "   - On the filtered set (say you’re down to 200 features), compute the mutual information `MI(X_i; Y)` for each remaining feature $X_i`.  \n",
    "   - Select the top 50 features based on MI.\n",
    "\n",
    "3. **Train a Model**  \n",
    "   - Now train your favorite classifier (e.g., random forest, logistic regression, etc.) using those 50 features.  \n",
    "   - Evaluate performance on a validation/test set.\n",
    "\n",
    "By combining multiple statistical measures (correlation, MI) in a pipeline, you can refine your feature set before feeding it to a predictive model.\n",
    "\n",
    "\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "- **Statistical filter** methods rely purely on measures like correlation, mutual information, chi-square, etc., to evaluate each feature’s relevance to the target.\n",
    "- They are generally **fast** and **simple** to implement, making them a common **first step** in feature selection.\n",
    "- They do **not** account for *interaction effects* among features, so sometimes advanced methods (wrapper or embedded) might be necessary.\n",
    "- In practice, you often combine filter methods with other selection techniques or domain knowledge to create a robust final feature set.\n",
    "\n",
    "\n",
    "### In Summary\n",
    "\n",
    "A **statistical filter** is a straightforward and efficient way to select important features by ranking them according to a statistical criterion. While they’re excellent at quickly reducing dimensionality and avoiding overfitting from too many features, be aware of their limitation in recognizing complex dependencies among features. They are typically the **first step** in a more extensive feature selection workflow for many machine learning projects.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "**Mutual Information (MI)** is a concept from **information theory** that measures the amount of information shared between two random variables. In other words, MI answers: *“How much does knowing one variable reduce the uncertainty about the other?”*\n",
    "\n",
    "Another way to say the same thing:\n",
    "**Mutual Information (MI)** measures how much knowing one random variable reduces uncertainty about another. If you know the outcome of one variable and it helps you predict the other, the mutual information is greater than zero.\n",
    "\n",
    "Zero MI ($I(X;Y) = 0$) means there is no dependence between $X$ and $Y$ (they are independent).\n",
    "High MI means knowing $X$ tells you a lot about $Y$, and vice versa.\n",
    "\n",
    "Here is a concise overview:\n",
    "\n",
    "### Basic Definition (Using Entropy)\n",
    "\n",
    "1. **Formula in Terms of Entropies**  \n",
    "   For two random variables $X$ and $Y$, the mutual information $I(X; Y)$ can be written as:\n",
    "\n",
    "   $$\n",
    "   I(X; Y) \\;=\\; H(X) \\;+\\; H(Y) \\;-\\; H(X, Y),\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $H(X)$ is the **entropy** of $X$ (a measure of the uncertainty in $X$).\n",
    "   - $H(Y)$ is the **entropy** of $Y$.\n",
    "   - $H(X, Y)$ is the **joint entropy** of $X$ and $Y$ together.\n",
    "\n",
    "2. **Equivalent Expressions**  \n",
    "   Mutual information can also be expressed in two other, but equivalent, ways:\n",
    "\n",
    "   - $$I(X; Y) = H(X) - H(X \\mid Y),$$\n",
    "   - $$I(X; Y) = H(Y) - H(Y \\mid X).$$\n",
    "\n",
    "   Here, $H(X \\mid Y)$ denotes the conditional entropy of $X$ given $Y$.  \n",
    "   Intuitively, $I(X; Y)$ measures how much the uncertainty in $X$ is reduced by knowing $Y$ (and vice versa).\n",
    "\n",
    "3. **Interpretation**  \n",
    "   - If $X$ and $Y$ are independent, then $H(X, Y) = H(X) + H(Y)$, making $I(X;Y) = 0$.  \n",
    "   - If $X$ can be fully determined by $Y$ (perfectly dependent), then $H(X \\mid Y) = 0$, and thus $I(X; Y) = H(X)$, the maximum possible reduction in uncertainty for $X$.\n",
    "\n",
    "Using these entropy-based definitions is often helpful when thinking about how mutual information relates to other information-theoretic quantities like **conditional entropy** and **joint entropy**.\n",
    "\n",
    "4. **Interpretation**  \n",
    "   - **Zero mutual information** means the variables do not share information about each other (i.e., they are statistically independent).  \n",
    "   - **High mutual information** indicates that knowing one variable gives a lot of information about the other (i.e., a strong dependency, whether linear or non-linear).\n",
    "\n",
    "5. **Comparison with Correlation**  \n",
    "   - **Correlation** measures only linear dependence and can miss complex relationships.  \n",
    "   - **Mutual information** captures *any* type of dependency (e.g., non-linear), which makes it a more general measure for determining whether two variables are related.\n",
    "\n",
    "6. **Practical Use in Machine Learning**  \n",
    "   - **Feature Selection**: In some methods, features that have high mutual information with the target (and potentially minimal redundancy with each other) are often chosen.  \n",
    "   - **Dimensionality Reduction**: Mutual information can help identify which variables or transformations preserve the most “information” relevant to the prediction.  \n",
    "   - **Distribution & Data Analysis**: Because MI deals with entire distributions (rather than just, say, means and variances), it can reveal relationships that simpler statistics overlook.\n",
    "\n",
    "7. **Estimation Challenges**  \n",
    "   - In practice, calculating MI directly from data requires estimating probability distributions (especially for continuous variables).  \n",
    "   - Techniques like **binning**, **kernel density estimation (KDE)**, or **k-nearest neighbors (KNN)** approaches are used to approximate these distributions.\n",
    "\n",
    "8. **Connection to Kullback–Leibler Divergence**  \n",
    "   - Another useful perspective is seeing MI as the **Kullback–Leibler (KL) divergence** between the joint distribution of X and Y, and the product of their marginal distributions:\n",
    "       \n",
    "     I(X; Y) = D_{KL}(p(X, Y) || p(X)p(Y)).\n",
    "       \n",
    "   - KL divergence tells us how one distribution differs from another, so mutual information tells us *how different* the joint distribution is from the assumption that \\(X\\) and \\(Y\\) are independent.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway**:  \n",
    "- Mutual information is a flexible measure of dependency between variables, capturing *any* relationship (not just linear). It’s often used in feature selection and understanding data relationships in machine learning, even though it can be more complex to estimate than simpler metrics like correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Missingness\n",
    "\n",
    "When dealing with datasets that have missing values, it’s important to understand why those values are missing. We usually categorize missingness into three main types:\n",
    "\n",
    "- **MCAR (Missing Completely At Random)**  \n",
    "  The probability of a value being missing is independent of both the observed data and the unobserved data.  \n",
    "  Example: A random glitch in a sensor that causes data to be lost sporadically, with no relation to the sensor’s actual reading.\n",
    "\n",
    "- **MAR (Missing At Random)**  \n",
    "  The probability of a value being missing depends on the observed data but **not** on the missing value itself.  \n",
    "  Example: A study participant fails to fill out certain questions *because of* their other known characteristics (like age or income), but not because of the unknown answer itself.\n",
    "\n",
    "- **MNAR (Missing Not At Random)**  \n",
    "  The probability of a value being missing depends on the *unobserved* data—i.e., the missing value itself.  \n",
    "  Example: Patients with more severe symptoms are more likely to skip follow-ups, so the data is *linked* to the severity level.\n",
    "\n",
    "**Why it matters**:  \n",
    "Different missingness mechanisms influence how you can safely impute or analyze data. Statistical methods often assume MCAR or MAR. MNAR requires specialized handling or modeling assumptions.\n",
    "  \n",
    "Key takeaways:\n",
    "- **Types of Missingness** (MCAR, MAR, MNAR) let you classify why data is missing, which affects how you handle it.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon Entropy\n",
    "\n",
    "**Shannon entropy** is a measure of **uncertainty** or **information content** in a **discrete** random variable $X$. If $X$ takes values $\\{x_1, x_2, \\dots, x_n\\}$ with probabilities $p(x_i)$, then the Shannon entropy $H(X)$ is:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{n} p(x_i) \\log_2 \\bigl(p(x_i)\\bigr).\n",
    "$$\n",
    "\n",
    "- **High entropy** means \\(X\\) is very unpredictable (many equally likely outcomes).  \n",
    "- **Low entropy** means \\(X\\) is more predictable (one or few outcomes dominate).  \n",
    "\n",
    "In information theory terms, $H(X)$ is the expected number of *bits* needed to encode the outcomes of $X$.\n",
    "  \n",
    "Key Takeaways:\n",
    "- **Shannon Entropy** measures uncertainty in discrete variables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Entropy\n",
    "\n",
    "For **continuous** random variables, the analogous concept is **differential entropy**. If $ X $ has probability density function $ f(x) $, the differential entropy $ h(X) $ is:\n",
    "\n",
    "$$\n",
    "h(X) = - \\int_{-\\infty}^{\\infty} f(x) \\, \\log \\bigl(f(x)\\bigr) \\, dx.\n",
    "$$\n",
    "\n",
    "**Key differences from Shannon entropy**:  \n",
    "- Differential entropy can be **negative**.  \n",
    "- It doesn’t have the same direct “bits” interpretation as the discrete case.  \n",
    "- It is not invariant under certain transformations (e.g., scaling the variable).\n",
    "  \n",
    "Key takeaways:\n",
    "- **Differential Entropy** is the continuous analog (to Shannon Entropy) but behaves differently than discrete entropy.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes’ Rule\n",
    "\n",
    "**Bayes’ Rule** describes how to update the probability of an event $ A $ after observing some evidence $ B $. Formally:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}.\n",
    "$$\n",
    "\n",
    "- $ P(A) $: **prior** probability of $ A $.  \n",
    "- $ P(A \\mid B) $: **posterior** probability (after seeing $ B $).  \n",
    "- $ P(B \\mid A) $: likelihood of observing $ B $ when $ A $ is true.  \n",
    "- $ P(B) $: overall (or marginal) probability of $ B $.\n",
    "\n",
    "This rule is fundamental in **Bayesian** statistics, enabling you to incorporate prior knowledge and then update your beliefs as new data arrives.\n",
    "  \n",
    "Key Takeaways:\n",
    "- **Bayes’ Rule** provides a framework for updating probabilities given new evidence and is central to Bayesian inference.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mina anteckningar från mötet med Jerrad 14/2 - 2025:\n",
    "\n",
    "A B C, mutual information capture bara main effekt, inte interactions mellan variables.\n",
    "Relief kollar på multipla variabler samtidigt. \n",
    "Relief har lite disadvantages också:\n",
    " man kan inte bara kolla på absolutvärdet, man måste kolla på multiple features för att kunna jämföra score.\n",
    "Använd MI för main effekts och relief för interactions\n",
    "Relief kan också detekta main effekt om den är simple / lijnär\n",
    "Orange: free to download, open source\n",
    "Reflief ger inte information om vilka interaktioner det finns mellan olika features.\n",
    "Man kan använda relief för tidsserie data också.\n",
    "\n",
    "Man kan använda MongoDB servern för tunga grejer.\n",
    "bashrc för att lägga till mongodb username och pswd\n",
    "installera mongodb compass\n",
    "compass har inbyggd mongosh\n",
    "man kan göra query direkt i compass\n",
    "aggregations\n",
    "man kan kolla som i tabell typ\n",
    "pymongo\n",
    "för att addera en df till mongodb, gör den först till dicts (to_dicts?) och sedan insertmany.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
